## Reference

[K-MOOC](http://www.kmooc.kr/courses/course-v1:SNUk+SNU049_011k+2018_T2/about)

[Youtube](https://www.youtube.com/channel/UCujyCcQ6GcInu-Wrha9ex_A)

### Market-Basket Rule

* **데이터**를 보고, 손님들이 어던 상품들을 주로 **같이 구매**하는지 알아낸다
  * Data: 모든 transaction이 있음
  * 주로 같이 구매하는 규칙(?): Association Rule

* 새로운 insight를 찾아낼 수 있다



### Frequent itemset

* 자주(빈발하게) **같이** 구매하는 아이템들
  * 예를 들어, (짱짱 유명한 예제인) diaper와 beer, milk를 같이 구매한 손님이 많은 경우 {diaper, beer, milk} 는 frequent itemset이 된다
* **자주** 의 기준: **최소 어떤 횟수(min support)** 이상 발생했을 경우



### Association Rule의 평가

* 거래 량이 많을 수록 Frequent itemset으로 알아낸 Association rule들이 무수하게 많이 나올 수 있다
* 그렇다면 이 Association rule들을 어떤 것이 중요하고, 어떤 것이 덜 중요한지 어떻게 평가할까?

$$
|Interest| = Conf(I→j) - Pr[j] ...... (1)
$$

$$
Conf(I→j) = Support(I ∪ j) / Support(I)......(2)
$$

* 수식
  * 평가할 때 쓰이는 직접적인 식은 1번 식이지만, 1번 식을 계산하기 위해서는 2번 식이 필요하다	
  * 2번 식은 I가 발생하였을 때 j가 발생한 확률을 계산하는 식으로, 분모에는 I가 발생한 모든 확률을, j에는 I와 j가 **함께** 바발생한 모든 support 수를 센다
  * 즉 여기서 구하고자 하는 것은, I가 발생할 때 j가 얼마나 (함께) 발생했느냐 인데, 만약 j가 I와 상관없이 자주 발생하는 사건일 수 있다. 이에 그런 경우를 제거해 줄 필요가 있다. 그것은 1번 식에서 구할 수 있다
  * 1번 식에서는, 2번 식에서 구한 Conf와 더불어 j가 단독으로 발생하는 빈도수 (support) 를 세준다
  * 이렇게 1번 식을 통해서 **어떤 Association rule이 중요한지** 알 수 있다 



### Itemset이 너무 많을 때

* Itemset이 너무 많다면, 그로 인해 나올 Association rule 또한 많아지게 된다
* 이에 Itemset의 갯수를 줄여줘야 할 필요가 있을 때, `Maximal frequent itemsets` 와 `Closed frequent itemsets` 를 통해 구한다.



### Maximal frequent itemsets

* 자신보다 더 큰 itemset이 없는 경우
* A, AB, BC, ABD 모두 frequent itemset일 때 maximal frequent itemset은 BC, ABD가 된다

| Table | Maximal frequent itemset |
| ----- | ------------------------ |
| A     | X                        |
| AB    | X                        |
| BC    | O                        |
| ABD   | O                        |



### Closed frequent itemset

* 자신보다 더 큰 itemset이 없으며, 그 itemset의 support 또한 최대일 때

| Table | Support | Closed frequent itemset |
| ----- | ------- | ----------------------- |
| A     | 3       | O                       |
| B     | 3       | X                       |
| BC    | 3       | O                       |
| ABC   | 2       | O                       |



### Association rule 구하기

1. 모든 Frequent itemset을 찾는다
2. 1에서 만든 Frequent itemset으로부터 Association rule을 도출한다

#### 예시

Frequent Itemset {j, m} 으로부터 j->m 또는 m->j 라는 식을 도출해내려고 한다.

Frequent itemset {j, m}을 I라고 하자. 이때 I의 subset이 되는 {j} 또는 {m}을 A라고 하자.

이 때 만드는 방법은
$$
A→I/A
$$

$$
{j} →{j, m}/j = j→m ......(1)
$$

$$
m→j,m/m=m→j......(2)
$$

이렇게 두개의 식이 나오게 된다. 이 때 **어느것을 Association rule로 할 것이냐** 는 **Conf** 로 판단하는데, 여기서 제시한 임계값 Conf는 **0.8** 이라 하고, (1)의 Conf는 1, (2)의 Conf는 0.8이라 가정한다.

**이에 따라 (1)번 식의 Conf가 임계 값인 0.8을 넘었으므로 (1) 식이 Association rule이 된다.**



### Frequent itemset 찾기

* 위에서 Association rule을 찾을 때, 모든 frequent itemset을 찾아야 한다고 하였다
* 그러면 모든 frequent itemset을 어떻게 찾을 수 있을까?

#### Naiive algorithm

* 모든 경우의 수를 세야 하기 때문에, 메모리 사용이 지나치게 많다

#### A-priori algorithm

* Naiive algorithm에서 지나치게 많은 메모리를 사용하는 것을 극복한 것으로, frequent 하지 않은 itemset을 알아내어 가지치고(pruning) 남은 것들로 count한다



## A-priori

* Frequent 하지 않은 itemset 들을 pruning 하기 위한 방법
* Multi-pass algorithm 이다 (각 pass는 disk I/O를 의미한다)

#### Motivation

* I⊃A 이고, I가 frequent itemset 이면 A도 그렇다
  * I: {n, m, p} 가 빈발하게 등장하면 I의 subset인 A: {n, m} 또한 빈발하게 등장한다
* 위 명제의 대우는 **A가 frequent itemset이 아니라면, I또한 frequent itemset이 아니다** 가 된다
* 이 명제에서 착안하여 만든 방식이 A-priori algorithm 이다



#### 예시

![Apriori 알고리즘](https://www.sites.google.com/site/getallcodesyouwant/_/rsrc/1318084435338/data-mining/apriori-algorithm/img054.jpg)



* Cn : Cn-1까지의 itemset을 봤을 때, frequent itemset이 될 가능성이 있는 itemset들의 모음
* Ln : Cn을 바탕으로 하여 알아낸 frequent itemset들의 모음



## PCY Algorithm

* A-priori 에서는 pass1일때 메모리가 지나치게 남고, pass2일때는 메모리를 굉장히 많이 사용한다는 문제점이 있었다.
* 이에 pass1에서 pass2의 일을 일부 하여, pass1의 idle한 memory를 사용하고 pass2에서 메모리 사용량을 줄이게끔 한다

![PCY algorithm](https://slideplayer.com/slide/5068663/16/images/5/PCY+Algorithm+%E2%80%93+Between+Passes.jpg)



* 이렇게 pass 1에서 frequent Itemset을 구한 후, 그에 대한 정보를 bitmap으로 압축하여 **어떤 Itmeset이 frequent itemset** 인지 pass2로 전달해준다
* 그러면 pass 2에서는 얘들을 가지고 candidate pair를 count한다



### 예시

1. Pass1의 Item count block 에서, 단순히 모든 item(당연히 k=1, {j}, {m}, {p} ..) 의 support 를 센다
2. 그리고 pass2의 Frequent item block에는 Pass1에서 구한 Item count를 바탕으로, min support 이상인 frequent itemset만 남게 된다. 
3. 그리고 Pass1의 Hash table block에서는 1.에서 itemset(k=1)들 중 minsup을 넘는 것들로 pair를 만들어서(k=2, {j, m}, {p, q} ...) 그들의 support를 구한 후 count를 한다.
   1. 그리고 그렇게 구해진 것들은 **bitmap** *(support를 저장하지 않고 어떤 itemset이 freq. itemset이냐(1) 아니냐(0)먄 저장하는 방법이다. 왜냐하면 freq. itemset이냐 아니냐가 중요하지, support는 중요하지 않기 때문이다)* 으로 바꿔서 Pass2의 Bitmap block에 저장된다
4. 그리고 Pass2의 Count of candidate pairs block에서는 (Pass2의)Bitmap block을 보고, all pair들의 support를 count한다. 이 때 확인할 조건은,
   1. 각 i, j가 freq. itemset 인지 {i} {j}
   2. {i, j}가 bitmap의 값이 1인지 (freq. itemset 인지와 같은 말)

* 즉 **이전 단계에서의 count(support)를 굳이 가지고 있지 않고, freq. itemset인지 여부만 가지고 있는다**



### Frequent itemsets in <2 passes

* disk I/O를 2번보다 적게 하여 frequent itemset을 찾는 방법

#### Random sampling algorithm

* Random하게 뽑아낸다
* 그리고 A-priori 실행



#### SON algorithm

* 전체 데이터를 k개의 chunk로 나누고, 각 chunk를 memory로 올린다

